# -*- coding: utf-8 -*-
"""
utils/integration_tr.py ‚Äì Exporta√ß√£o/Importa√ß√£o do TR
Respons√°vel por:
- Gravar o arquivo exports/tr_data.json a partir dos metadados do TR.
- Ler o arquivo exports/tr_data.json para pr√©-preencher o m√≥dulo Contrato.
"""

import json
import os
import re
from typing import Dict, Any
from pathlib import Path
from utils.ai_client import AIClient

# ==========================================================
# üìÇ Diret√≥rios e caminhos de exporta√ß√£o
# ==========================================================
EXPORTS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "exports")
TR_JSON_PATH = os.path.join(EXPORTS_DIR, "tr_data.json")

client = AIClient()

# ==========================================================
# üì§ Utilit√°rios de exporta√ß√£o
# ==========================================================
def ensure_exports_dir(path: str = EXPORTS_DIR) -> None:
    os.makedirs(path, exist_ok=True)

def export_tr_to_json(data: Dict[str, Any], path: str = TR_JSON_PATH) -> str:
    ensure_exports_dir(os.path.dirname(path))
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

def load_tr_from_json(path: str = TR_JSON_PATH) -> Dict[str, Any]:
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

# ==========================================================
# üß† Base de conhecimento institucional (Knowledge Base)
# ==========================================================
def ler_modelos_tr() -> str:
    """L√™ os modelos textuais da pasta knowledge/tr_models."""
    base = Path(__file__).resolve().parents[1] / "knowledge" / "tr_models"
    textos = []
    if base.exists():
        for arq in base.glob("*.txt"):
            try:
                textos.append(arq.read_text(encoding="utf-8"))
            except Exception:
                pass
    return "\n\n".join(textos)

# ==========================================================
# ü§ñ Processamento de Insumo ‚Äì IA Institucional TR
# ==========================================================
def processar_insumo_tr(arquivo, artefato: str = "TR") -> dict:
    """
    Extrai o texto do arquivo enviado (PDF, DOCX ou TXT),
    realiza an√°lise sem√¢ntica e retorna campos padronizados do TR.
    """
    from io import BytesIO
    import fitz, docx2txt

    dados = arquivo.read()
    arquivo.seek(0)
    nome = arquivo.name.lower()
    texto_extraido = ""

    # 1Ô∏è‚É£ Extra√ß√£o de texto
    try:
        if nome.endswith(".pdf"):
            pdf = fitz.open(stream=dados, filetype="pdf")
            texto_extraido = "".join(p.get_text() for p in pdf)
        elif nome.endswith(".docx"):
            texto_extraido = docx2txt.process(BytesIO(dados))
        elif nome.endswith(".txt"):
            texto_extraido = dados.decode("utf-8", errors="ignore")
    except Exception as e:
        return {"erro": f"Falha ao extrair texto: {e}"}

    if not texto_extraido.strip():
        return {"erro": "Texto vazio ap√≥s leitura do insumo."}

    texto_limpo = re.sub(r"\s+", " ", texto_extraido).strip()
    modelos = ler_modelos_tr()

    # 2Ô∏è‚É£ Prompt institucional
    system_prompt = (
        "Voc√™ √© um agente institucional do Tribunal de Justi√ßa de S√£o Paulo, especializado em Termos de Refer√™ncia (TR). "
        "Analise o texto do insumo e extraia os campos padronizados conforme os modelos institucionais do TJSP."
    )

    user_prompt = f"""
Texto do insumo:
\"\"\"{texto_limpo}\"\"\"

Modelos de refer√™ncia:
\"\"\"{modelos}\"\"\"

Retorne apenas um JSON com os seguintes campos:
- objeto
- justificativa
- especificacoes_tecnicas
- criterios_de_julgamento
- obrigacoes_da_contratada
- prazo_execucao
- estimativa_valor
- fonte_recurso
"""

    # 3Ô∏è‚É£ Chamada √† IA institucional
    try:
        response = client.chat([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ])
        conteudo = response["content"]
        match = re.search(r"\{.*\}", conteudo, re.DOTALL)
        campos = json.loads(match.group(0)) if match else {"objeto": texto_limpo[:800]}
    except Exception as e:
        campos = {"erro": f"Falha ao processar IA: {e}"}

    # ==========================================================
    # üîÑ Normaliza√ß√£o de campos para compatibilidade com a p√°gina TR
    # ==========================================================
    campos_ai = {
        "objeto": campos.get("objeto", ""),
        "justificativa_tecnica": campos.get("justificativa", ""),
        "especificacao_tecnica": campos.get("especificacoes_tecnicas", ""),
        "criterios_julgamento": campos.get("criterios_de_julgamento", ""),
        "riscos": campos.get("obrigacoes_da_contratada", "Sem riscos adicionais identificados."),
        "observacoes_finais": "",
        "prazo_execucao": campos.get("prazo_execucao", ""),
        "estimativa_valor": campos.get("estimativa_valor", ""),
        "fonte_recurso": campos.get("fonte_recurso", "")
    }

    # Fallback seguro
    for k, v in campos_ai.items():
        if not v:
            campos_ai[k] = "‚Äî"

    print(f"[IA:TR] Arquivo: {arquivo.name} ‚Äì Campos normalizados: {list(campos_ai.keys())}")

    # ==========================================================
    # üì¶ Retorno final compat√≠vel com o SynapseNext
    # ==========================================================
    return {
        "artefato": artefato,
        "nome_arquivo": arquivo.name,
        "status": "processado",
        "campos_ai": campos_ai
    }
